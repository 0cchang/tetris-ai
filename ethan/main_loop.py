# import necessary libs

# Create DQN model and initialize parameters

# Create environment
# - specify simplified action space

# Make first observation about game state

# Randomly select action to take

# Get new game state with reward/loss and store in replay buffer

# Set this new game state as the starting state

'''
For however many episodes we want
    Enter the starting state
    Select Best action (epsilon greedy method)
    Perform action
    Get new game state with reward/loss and store in replay buffer
    state = new state
'''
    